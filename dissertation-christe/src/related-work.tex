\chapter{Related Work}\label{ch:related-work}
This chapter reviews research related defining Big Data in terms of DSNs, Big Data management, self-optimizing DSNs, predictive analytics and forecasting, optimizations to triggering, detection, and classification of signals-of-interest within the context of DSNs.

\section{Big Data and Distributed Sensor Networks}\label{sec:big-data-and-distributed-sensor-networks}
``Big Data" is a term that is used to define either the characteristics of collected data or the processes involved for storing and analyzing collected data. Information that is considered Big Data provides a number of challenges.

One of the best reviews on Big Data literature is provided by the President's Council of Advisors on Science and Technology (PCAST) in their report to the White House~\cite{house2014big}. In this review, Big Data is described using several definitions.

The first definition includes ``high-volume, high velocity, and high-variety information assets that demand cost-effective, innovative forms of information processing for enhanced insight and decision making"~\cite{gartner_it_glossary_2016}. This definition focuses on the characteristics of the data that make it ``Big". In this context high-volume refers to the total amount of data that requires processing, high-velocity refers to the speed at which data arrives, and high-variety refers to the fact that sensor data is often heterogeneous and incomplete. The second part of the definition includes the terms cost-effective, innovative forms of information processing for enhanced insight and decision processing, which implies that we need technology that is able to deal with these types of data characteristics while doing so within the limits of a system with the goal of refining the data to provide insights and decision making that would not have been possible without the information processing.

A second definition~\cite{ward2013undefined} mentioned by the PCAST report rings more true to what Laha attempts to accomplish within the context of DSNs and says that Big Data is ``a term describing the storage and analysis of large and/or complex data sets using a series of techniques including, but not limited to, NoSQL, MapReduce, and machine learning". This second definition defines Big Data in terms of storage and analysis techniques and is a useful definition for describing the processes by which Laha and the Laha reference DSNs deal with distributed sensor data.

Finally, Bhat~\cite{bhat2018data} shows that the production of ``Big Data" is greatly outpacing the available storage for that data. The author shows evidence that even with technological advances is data storage mediums, that the gap still exists. Further, the author shows that there are several shortcomings with current data reduction techniques such as compression and deduplication. Issues with compression include that fact that while lossy compression provides better compression ratios, data quality is reduced and using lossless compression algorithms does not provide the required compression ratios. Compression also provides additional overhead in terms of CPU utilization. Similar to compresses, deduplication shifts the costs from the network to the CPU. As such, other techniques should be considered for data reduction.

\section{Distributed Sensor Networks and Big Data Management}\label{sec:distributed-sensor-networks-and-big-data-management}

Garcia-Gil et al.~\cite{garcia2019enabling} introduce the concept of ``Smart Data" which is value and veracity added to data by means of removing noise from ``Big Data". The authors claim that applying labels to data through classification algorithms are more accurate when noisy data is removed from the initial data set. In order to remove noise, the authors train two different models against noisy and non-noisy data. The authors provided two different approaches to this process both using Apache Spark to distribute the processing load. The first approach the Homogeneous Ensemble for Big Data (HME-BD) which uses a single random forest classifier. The authors also provided a second approach called the Heterogeneous Ensemble for Big Data (HTE-BD) which utilized multiple classifiers for identifying noisy data, namely random forests, logistic regression, and k-nearest neighbors. The authors found the HME-BD approach provided more accurate results and better performance characteristics.  The authors found that by removing noisy data, they were able to increase the accuracy of the classification algorithms.

There are many technologies for movement, transformation, and storage of sensor data. Current state of the art technologies include distributed streaming and computation engines such as Apache Kafka~\cite{kreps2011kafka} or Apache NiFi~\cite{noauthor_apache_nodate}. Although these frameworks provide a lot of flexibility in terms of transformations applied and data management, they do not provide automatic mechanisms for data management. Other, less known technologies are discussed in~\cite{hughes2016survey}, but also suffer from the fact that they are flexible in moving large amount of data, but do nothing to address storage requirements or graceful degradation.

Another approach is to use compression techniques, such as those described by Tang~\cite{tang2004compression}. Tang utilizes spatio-temporal correlation to reduce the amount of data that is transferred from a set of distributed sensors. Tang uses these application specific algorithms to reduce the overall size by a factor of 8 while still maintaining the target signal-to-noise ratio required by the network. However, at scale, even data compression can not keep up with the approach of storing everything all the time.

There are many distributed computation engines and techniques which provide a generic framework for distributing computational tasks across multiple CPUs and multiple machines. The two, which are generally receiving the most academic attention are MapReduce~\cite{dean2008mapreduce} and Apache Spark~\cite{zaharia2016apache}. Although these computation engines are very generic and quite powerful, they can not easily inherit any of the optimizing benefits provided by Phenomena in the Laha framework.

The data grid~\cite{chervenak2000data} is a framework that was designed to provide two basic services the authors believe are fundamental for distributed management and analysis of large scientific datasets, storage systems and metadata management.

Wu~\cite{wu2014data} constructs the HACE framework, which is specifically designed for mining of insightful data from varied Big Data sets. Although this framework is useful for managing multiple streams of data and mining over multiple features, it does not attempt to provide an upper bounds on storage requirements or provide graceful degradation in the face of large scaling networks.

In terms of frameworks using aggregation to facilitate data reduction, Camdoop designed by Costa~\cite{costa2012camdoop} is a framework that aims to push aggregation techniques from the edge of the sensor network all the way to the sink. Camdoop was able to show positive results in data reduction while still maintaining semantic meaning. However, Camdoop was designed to run over simple data streams (such as word count logs) and it is not known how this system would perform with more primitive types of data. Camdoop was designed to run within CamCube simulations and it's not known how this would run in practice with a real DSN.

Rehman et al.~\cite{ur2016big} created a big data reduction framework and argue that reducing data early in the analytics process can lead to efficient value creation. This framework was designed specifically for enterprise customer Big Data analysis, but I believe some of the core tenants could apply to any Big Data problem. They argue that by performing data reduction early in the process its possible to lower service utilization costs, enhance trust between users and developers, and preserve privacy of users among other benefits.

Luan et al.~\cite{luan2015fog} in their paper on Fog Computing, describe data reduction and aggregation techniques by performing some of a subset of computations and data reduction on the edge of the network, such as in mobile devices (cellphones) or in servers that geographically located near the data acquisition sources. Aggregated data is then sent from the edge devices to data sinks for further analysis or action. One of the major difficulties with this approach is handling scale and being able to dynamically deploy resources to the edge as data streams scale.

In a paper by Stateczny et al.~\cite{stateczny2014self}, the authors work to determine whether artificial neural networks can be used to provide Big Data Reduction for hydrographic sonar data. The authors found that they were able to see some reduction, but ran into issues when the data was very dense. The research presented here also appears to be very domain specific.

\section{Distributed Sensor Networks and Predictive Analytics and Forecasting}\label{sec:distributed-sensor-networks-and-predictive-analytics-and-forecasting}

Mohsenian-Rad et al.~\cite{mohsenian2018distribution} designed the $\mu$PMU (phase measurement unit) system which provides distributed power quality measurements over power grid distribution systems. The $\mu$PMUs in conjunction with their backend software provide two types of analytics. Descriptive analytics provide information about the types and classifications of power quality issues that are observed within the power distribution grid. Predictive analytics are used to predict future power quality issues. The authors describe their system as providing the ground for for enabling future prescriptive analytics, which is the idea of self-tuning the DSN to prepare for future power quality problems by using a combination of descriptive and predictive analytics. Prescriptive analytics are a concept that currently exist within the Laha framework.

Anastasi et al.~\cite{anastasi_energy_2009} breaks data predictions algorithms for DSNs into two classes. The first class of algorithms are defined as stochastic approaches and use random probabilities and non-precise statistics to provide predictions. The other class is called time series forecasting and uses historical time series data to provide future predictions. An example of a stochastic model for predicting sensor data is the Ken model~\cite{chu2006approximate} which was developed for energy reduction by minimizing the data sent between sensors and sink nodes. This is accomplished by using a model of sensed data and only sending data when the sensed values at the sensor do not match what was predicted by the model. The model is built during a training phase in which a probabilistic density function (PDF) is generated for the model. Ken is flexible enough to provide models for different types of sensed phenomenon and can work anywhere where there are high correlations in time and space.

Time series forecasting algorithms typically use moving average, auto regressive, or auto regressive moving average models. The authors of the PAQ framework~\cite{tulone2006paq} use auto-regression techniques to build a model of sensor readings that is compared between sensor node and sink nodes while  providing provably correct error bounds. The SAF architecture~\cite{tulone2006energy}, by the same authors, improves on the PAQ framework by refining the AR models and also adds the ability to not only detect outliers, but also detect inconsistent data. These approaches provide predictions for a single feature, however Laha provides the ability for DSNs to be multi-modal. The paper presented by Le et al.~\cite{le2007adaptive} uses time series forecasting, but provides multiple models that are switched out when the data changes. That is, given the current state of the network, a model is selected that is most likely to provide correct predictions. This is useful if a network has multiple features that can be used for forecasting.

Han et al.~\cite{han1999efficient} create an approach for efficient mining of partial periodic patterns in time series databases. Research before this could only match periodic signals if the patterns were completely full, however the authors augment this approach to support the finding of partial periodic signals, which are more common in practice. The authors show that the signals can be recognized after 2 passes of the database. Keogh et al.~\cite{keogh2002finding} take a different approach with their Tarzan algorithm and instead of mining for known periodic signals, they come up with an approach to enumerate all ``surprising" patterns of data in time series databases. They use a statistical approach (a subset of the stochastic approach) that works in linear time to determine if the occurrence of a data point differs from that expected by change. They found that their approach was more sensitive and selective than other approaches described in the paper.

\section{Determining Topology and Localization}\label{sec:determining-topology-and-localization}

Several recent papers have been published for determining the location source of infrasonic signals of interest. These include papers by Pilger et al.~\cite{pilger2019large} for tracking large meteoroids using the International Monitoring System (IMS), Hupe et al.~\cite{hupe2019can} for utilizing IMS to contribute to gravity wave measurements, and Farges et al.~\cite{farges2019infrasound} for localization of lightning strikes using infrasound detections through the IMS. The one commonality between all of these research projects is that they utilize the IMS system for collecting their measurements. The IMS is a large, multi-national effort for creating a high uptime network for detecting nuclear proliferation. This dissertation will show the viability of creating an inexpensive DSN that rivals the IMS using common off the shelf equipment.

Langendoen and Reijers~\cite{langendoen2003distributed} provide comparisons for localization techniques of large DSNs. This approach requires that the DSNs are self organizing and do not depend on global infrastructure (such as GPS), are tolerant to node failures, and are energy efficient. These constraints rule out other localization approaches such as GPS. One thing that differentiates Laha networks to Langendoen's is that Langendoen assumes a random distribution of sensor nodes where sensors in Laha networks are strategically placed. If there are a fraction of nodes that do know their location (anchor nodes), then there are several techniques that meet Langendoen's criteria including Ad hoc Positioning System from Niculescu et al.~\cite{niculescu2003ad}, the N-hop Multilateration Primitive by Savvides et al.~\cite{savvides2002bits}, and Rabaey's work on robust positioning algorithms~\cite{rabaey2002robust}. The three approaches all use three similar phases for localization: distance between anchor nodes and other sensors, position, and refinement. Laha hopes to provide sensor distance between sensors rather than physical distance. The above algorithms use flooding of the network for evaluate distance metrics, which may not be possible in Laha deployed networks.

When timing synchronization between nodes is sufficient, that is, the synchronization between sensors provides a timing accuracy of more than the Nyquist frequency for the signals of interest trying to be captured, it is possible to use arrival time of signals to provide metrics on sensing field topology and localization. This is the premise behind sets of algorithms that look at a single signal and the arrival times of that signal at multiple sensors along with possible direction and then attempt to provide an estimate of source signal localization. This has been performed in infrasound networks using the INFERNO framework as described by Perttu~\cite{perttu2013regional} and in other acoustic DSNs such as those used for efficient shooter localization (finding the source of a gun shot from collected acoustic signatures) in~\cite{gezici2005localization} and~\cite{maroti2004shooter}. Localization of non-acoustic signals has also been shown in the literature. For example, Parsons et al. provide a method for localizing PQ disturbances by analyzing energy flow and peak instantaneous power for both capacitor energizing  and voltage sag disturbances from sampled voltage and current data~\cite{parsons1998direction}.

Although not related to determining the topology of a PQ network, there is research that can also find the optimal placement of PQ sensors given a the topology of the network. Won, et al.~\cite{won2008optimal} provide an automatic method of placing PQ sensors on a known topology to maximize signal collection while minimizing the total number of required sensors.

\section{Optimizations for Triggering}\label{sec:optimizations-for-triggering}
Triggering is the act of observing a feature extracted data stream for interesting features and triggering sensors to provide raw data for a requested time window for higher level analysis. Adaptively optimizing triggering is a way to tune triggering algorithms and parameters with the aim of decreasing false positives and false negatives. In this context, a false positive is triggering on a data stream that does not contain a signal of interest and a false negative is not triggering on a data stream that does contain a signal of interest.

Many of the optimizing triggering algorithms present in the literature exist to minimize sensor energy requirements and bandwidth requirements. This is addressed in great detail in the literature review by Anastasi et al.~\cite{anastasi_energy_2009}. This is accomplished by reducing communications between sensor nodes and the sink. It is argued in~\cite{pottie2000wireless} that the costs of transmitting a single bit of information from a sensor cost approximately the same as running 1000 operations on that sensor. However, there is some contention on this topic as~\cite{alippi_adaptive_2010} argues that in some modern sensors computational requirements can equal or eclipse those of  sensor communication.

One of the main drivers of optimization of triggering is to take advantage of the known sensing field topology of a DSN. This is often referred to in the literature as ``topology control"~\cite{santi2005topology}. When the topology of the sensing field is known and when there is an adequate density of sensors, Vuran et al.~\cite{luan2015fog} show that sampled data display strong spatial and temporal correlations. This fact can be used to reduce the amount of duplicate sensor data that is transmitted, stored, and processed. Topology control is generally split into two categories, ``location  driven" where the location of the sensor is known and ``connectivity driven" which aims to dynamically activate or deactivate sensors to provide complete coverage of a sensing field. Many of the location based approaches in the literature attempt  to maximize the ability for sensors to communicate with each other, however Laha takes the approach that all sensors communicate directly with sink nodes eliminating the need for optimizing intra-sensor communications. One downside to location based approaches is that GPS sensors can be energy hogs and only work with directly line of site to the atmosphere. In these cases, a subset of sensors can be supplied with a GPS and the other sensor use additional techniques such as NTP or statistical analysis to determine location~\cite{langendoen2003distributed}.

More details on topology control can be gathered in the reviews by Karl et al.~\cite{karl2007protocols} and Vuran et al.~\cite{vuran2004spatio}.







